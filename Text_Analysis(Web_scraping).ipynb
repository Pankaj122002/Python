{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOOqTPKxIFkF3XSMwK0DaHM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pankaj122002/Python/blob/main/Text_Analysis(Web_scraping).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSsjIZYSh7nf",
        "outputId": "ac45a4b5-9abd-4311-947f-ac42f475798a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2023.7.22)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.4.1)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.10/dist-packages (from textblob) (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (8.1.6)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (4.66.1)\n",
            "Collecting syllables\n",
            "  Downloading syllables-1.0.7-py3-none-any.whl (15 kB)\n",
            "Collecting cmudict<2.0.0,>=1.0.11 (from syllables)\n",
            "  Downloading cmudict-1.0.13-py3-none-any.whl (939 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m939.3/939.3 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting importlib-metadata<6.0.0,>=5.1.0 (from syllables)\n",
            "  Downloading importlib_metadata-5.2.0-py3-none-any.whl (21 kB)\n",
            "Collecting importlib-resources<6.0.0,>=5.10.1 (from cmudict<2.0.0,>=1.0.11->syllables)\n",
            "  Downloading importlib_resources-5.13.0-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<6.0.0,>=5.1.0->syllables) (3.16.2)\n",
            "Installing collected packages: importlib-resources, importlib-metadata, cmudict, syllables\n",
            "  Attempting uninstall: importlib-resources\n",
            "    Found existing installation: importlib-resources 6.0.1\n",
            "    Uninstalling importlib-resources-6.0.1:\n",
            "      Successfully uninstalled importlib-resources-6.0.1\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 6.8.0\n",
            "    Uninstalling importlib-metadata-6.8.0:\n",
            "      Successfully uninstalled importlib-metadata-6.8.0\n",
            "Successfully installed cmudict-1.0.13 importlib-metadata-5.2.0 importlib-resources-5.13.0 syllables-1.0.7\n"
          ]
        }
      ],
      "source": [
        "!pip install requests\n",
        "!pip install beautifulsoup4\n",
        "!pip install textblob\n",
        "!pip install syllables"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from textblob import TextBlob\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RLssXW04rG3U",
        "outputId": "b30bb290-9575-4757-bf5e-3417459df543"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_df = pd.read_excel('/content/Input.xlsx')\n",
        "\n",
        "results = []\n",
        "\n",
        "# Iterate each row in input DataFrame\n",
        "for index, row in input_df.iterrows():\n",
        "    url_id = row['URL_ID']\n",
        "    url = row['URL']\n",
        "    def extract_article_text(url):\n",
        "        response = requests.get(url)\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    #extract the article title and text\n",
        "        article_text = \"\"\n",
        "        article_title = soup.find('title').get_text()\n",
        "        paragraphs = soup.find_all(['p', 'div'])\n",
        "        for p in paragraphs:\n",
        "            article_text += p.get_text()\n",
        "        return article_title, article_text\n",
        "    article_title, article_text = extract_article_text(url)\n",
        "\n",
        "    # Perform text analysis using TextBlob\n",
        "    b = TextBlob(article_text)\n",
        "    positive_score = b.sentiment.polarity\n",
        "    negative_score = b.sentiment.polarity\n",
        "    polarity_score = b.sentiment.polarity\n",
        "    subjectivity_score = b.sentiment.subjectivity\n",
        "    avg_sentence_length = len(b.words) / len(b.sentences)\n",
        "    import re\n",
        "    def count_syllables(word):\n",
        "    # Convert word to lowercase\n",
        "        word = word.lower()\n",
        "\n",
        "    # Define a regular expression to match consecutive vowels\n",
        "    # but not ending with 'e'\n",
        "        pattern = '(?!e$)[aeiouy]+'\n",
        "\n",
        "    # Find all matches of the pattern in the word\n",
        "        matches = re.findall(pattern, word, re.I)\n",
        "\n",
        "    # Return the count of matches (syllables)\n",
        "        return len(matches)\n",
        "\n",
        "\n",
        "    word = \"syllable\"\n",
        "    syllable_count = count_syllables(word)\n",
        "\n",
        "    def count_complex_words(b):\n",
        "        complex_word_count = sum(1 for word in b.words if syllable_count >= 3)\n",
        "        return complex_word_count\n",
        "\n",
        "    complex_word_count = count_complex_words(b)\n",
        "\n",
        "    percentage_complex_words = (complex_word_count / len(b.words)) * 100\n",
        "\n",
        "    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
        "\n",
        "    avg_words_per_sentence = len(b.words) / len(b.sentences)\n",
        "\n",
        "    word_count = len(b.words)\n",
        "\n",
        "    avg_syllables_per_word = syllable_count / word_count\n",
        "\n",
        "    personal_pronoun_count = sum(1 for word in b.words if word.lower() in ['i', 'you', 'he', 'she', 'we', 'they'])\n",
        "\n",
        "    avg_word_length = sum(len(word) for word in b.words) / word_count\n",
        "\n",
        "\n",
        "    result = {\n",
        "        'URL_ID': url_id,\n",
        "        'URL' : url,\n",
        "        'POSITIVE SCORE': positive_score,\n",
        "        'NEGATIVE SCORE': negative_score,\n",
        "        'POLARITY SCORE': polarity_score,\n",
        "        'SUBJECTIVITY SCORE': subjectivity_score,\n",
        "        'AVG SENTENCE LENGTH': avg_sentence_length,\n",
        "        'PERCENTAGE OF COMPLEX WORDS': percentage_complex_words,\n",
        "        'FOG INDEX':fog_index,\n",
        "        'AVG NUMBER OF WORDS PER SENTENCE' :avg_words_per_sentence,\n",
        "        'COMPLEX WORD COUNT': complex_word_count,\n",
        "        'WORD COUNT': word_count,\n",
        "        'SYLLABLE PER WORD': avg_syllables_per_word,\n",
        "        'PERSONAL PRONOUNS': personal_pronoun_count,\n",
        "        'AVG WORD LENGTH': avg_word_length\n",
        "          }\n",
        "    results.append(result)\n",
        "output_df = pd.DataFrame(results)\n",
        "output_df.to_excel('output.xlsx', index=False)"
      ],
      "metadata": {
        "id": "ygRd11Dkuorg"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ljN9sF1RAD8f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}